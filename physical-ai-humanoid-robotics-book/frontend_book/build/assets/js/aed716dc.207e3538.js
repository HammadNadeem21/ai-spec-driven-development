"use strict";(globalThis.webpackChunkfrontend_book=globalThis.webpackChunkfrontend_book||[]).push([[108],{8453(n,e,t){t.d(e,{R:()=>o,x:()=>r});var a=t(6540);const i={},s=a.createContext(i);function o(n){const e=a.useContext(s);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:o(n.components),a.createElement(s.Provider,{value:e},n.children)}},9427(n,e,t){t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"modules/vla/chapter-2-cognitive-planning","title":"Chapter 2: Language-Driven Cognitive Planning","description":"Overview","source":"@site/docs/modules/vla/chapter-2-cognitive-planning.md","sourceDirName":"modules/vla","slug":"/modules/vla/chapter-2-cognitive-planning","permalink":"/docs/modules/vla/chapter-2-cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/vla/chapter-2-cognitive-planning.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Chapter 2: Language-Driven Cognitive Planning"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 1: Voice-to-Action with Speech Models","permalink":"/docs/modules/vla/chapter-1-voice-to-action"},"next":{"title":"Chapter 3: Capstone \u2013 The Autonomous Humanoid","permalink":"/docs/modules/vla/chapter-3-autonomous-humanoid"}}');var i=t(4848),s=t(8453);const o={sidebar_position:2,title:"Chapter 2: Language-Driven Cognitive Planning"},r="Chapter 2: Language-Driven Cognitive Planning",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Using LLMs for Task Decomposition",id:"using-llms-for-task-decomposition",level:2},{value:"Cognitive Planning Architecture",id:"cognitive-planning-architecture",level:3},{value:"LLM Prompt Engineering for Robotics",id:"llm-prompt-engineering-for-robotics",level:3},{value:"Translating Natural Language into ROS 2 Action Sequences",id:"translating-natural-language-into-ros-2-action-sequences",level:2},{value:"Example Translation Process",id:"example-translation-process",level:3},{value:"ROS 2 Action Mapping",id:"ros-2-action-mapping",level:3},{value:"Safety and Constraint-Aware Planning",id:"safety-and-constraint-aware-planning",level:2},{value:"Safety Validation Framework",id:"safety-validation-framework",level:3},{value:"Integration with ROS 2 Safety Systems",id:"integration-with-ros-2-safety-systems",level:3},{value:"Implementation Best Practices",id:"implementation-best-practices",level:2},{value:"Error Handling and Fallbacks",id:"error-handling-and-fallbacks",level:3},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Summary",id:"summary",level:2}];function p(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"chapter-2-language-driven-cognitive-planning",children:"Chapter 2: Language-Driven Cognitive Planning"})}),"\n",(0,i.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(e.p,{children:"This chapter focuses on using Large Language Models (LLMs) for cognitive planning in humanoid robotics. We'll explore how to translate natural language commands into executable ROS 2 action sequences while implementing safety and constraint-aware planning mechanisms."}),"\n",(0,i.jsx)(e.h2,{id:"using-llms-for-task-decomposition",children:"Using LLMs for Task Decomposition"}),"\n",(0,i.jsx)(e.p,{children:"Large Language Models have revolutionized the ability to understand and decompose complex natural language commands into structured action sequences. In robotics, this capability enables more intuitive human-robot interaction by allowing users to express complex tasks in natural language."}),"\n",(0,i.jsx)(e.h3,{id:"cognitive-planning-architecture",children:"Cognitive Planning Architecture"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import openai\nimport time\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any\nimport json\n\n@dataclass\nclass ActionSequence:\n    """Represents a single action in a task plan that can be executed by the robot"""\n    id: str\n    type: str  # navigation, manipulation, perception, etc.\n    parameters: Dict[str, Any]\n    dependencies: List[str]\n    timeout: float\n    priority: int\n\n@dataclass\nclass TaskPlan:\n    """Represents a decomposed plan generated from a natural language command"""\n    id: str\n    original_command: str\n    actions: List[ActionSequence]\n    constraints: List[Dict[str, Any]]\n    status: str  # pending, executing, completed, failed\n    created_at: str\n\n@dataclass\nclass SafetyConstraint:\n    """Represents a safety constraint that must be validated before executing actions"""\n    id: str\n    type: str  # environmental, physical, operational\n    condition: str\n    severity: str  # warning, error\n    description: str\n\nclass CognitivePlanningSystem:\n    def __init__(self, model_name="gpt-3.5-turbo"):\n        self.client = openai.OpenAI()\n        self.model_name = model_name\n\n    def decompose_task(self, command_text: str) -> TaskPlan:\n        """Decompose a natural language command into executable actions"""\n        prompt = f"""\n        Decompose the following command into a sequence of ROS 2 actions for a humanoid robot:\n        Command: "{command_text}"\n\n        Return a JSON object with the following structure:\n        {{\n            "actions": [\n                {{\n                    "id": "action_id",\n                    "type": "action_type",\n                    "parameters": {{"param1": "value1"}},\n                    "dependencies": ["dependency_id"],\n                    "timeout": 10.0,\n                    "priority": 5\n                }}\n            ],\n            "constraints": [\n                {{\n                    "id": "constraint_id",\n                    "type": "constraint_type",\n                    "condition": "condition_string",\n                    "severity": "error",\n                    "description": "constraint_description"\n                }}\n            ]\n        }}\n\n        Action types should be from: navigation, manipulation, perception, communication.\n        Ensure all actions are safe and executable by a humanoid robot.\n        """\n\n        try:\n            response = self.client.chat.completions.create(\n                model=self.model_name,\n                messages=[{"role": "user", "content": prompt}],\n                response_format={"type": "json_object"}\n            )\n\n            result = json.loads(response.choices[0].message.content)\n\n            # Create action sequences\n            actions = []\n            for action_data in result.get("actions", []):\n                action = ActionSequence(\n                    id=action_data["id"],\n                    type=action_data["type"],\n                    parameters=action_data["parameters"],\n                    dependencies=action_data.get("dependencies", []),\n                    timeout=action_data.get("timeout", 10.0),\n                    priority=action_data.get("priority", 5)\n                )\n                actions.append(action)\n\n            # Create task plan\n            task_plan = TaskPlan(\n                id=f"plan_{int(time.time())}",\n                original_command=command_text,\n                actions=actions,\n                constraints=result.get("constraints", []),\n                status="pending",\n                created_at=time.time()\n            )\n\n            return task_plan\n\n        except Exception as e:\n            print(f"Error in task decomposition: {e}")\n            # Return a default task plan indicating failure\n            return TaskPlan(\n                id=f"plan_{int(time.time())}",\n                original_command=command_text,\n                actions=[],\n                constraints=[],\n                status="failed",\n                created_at=time.time()\n            )\n'})}),"\n",(0,i.jsx)(e.h3,{id:"llm-prompt-engineering-for-robotics",children:"LLM Prompt Engineering for Robotics"}),"\n",(0,i.jsx)(e.p,{children:"Effective task decomposition requires careful prompt engineering to ensure the LLM understands robotics-specific constraints and capabilities:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Context Provision"}),": Provide information about the robot's capabilities and limitations"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Safety Constraints"}),": Include safety guidelines in the prompt to prevent dangerous actions"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Action Vocabulary"}),": Define a standardized set of action types the robot can perform"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Format Consistency"}),": Ensure consistent JSON output for reliable parsing"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"translating-natural-language-into-ros-2-action-sequences",children:"Translating Natural Language into ROS 2 Action Sequences"}),"\n",(0,i.jsx)(e.p,{children:"The translation from natural language to executable actions involves several steps:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Intent Recognition"}),": Understanding the user's high-level goal"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Task Decomposition"}),": Breaking down the goal into sequential actions"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Action Mapping"}),": Converting abstract actions to specific ROS 2 services/topics"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Constraint Validation"}),": Ensuring actions comply with safety and operational constraints"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"example-translation-process",children:"Example Translation Process"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class NaturalLanguageTranslator:\n    def __init__(self):\n        self.planning_system = CognitivePlanningSystem()\n        self.action_mapper = ROS2ActionMapper()\n\n    def translate_command(self, command: str) -> List[ActionSequence]:\n        """Translate natural language command to ROS 2 action sequences"""\n        # Step 1: Decompose task using LLM\n        task_plan = self.planning_system.decompose_task(command)\n\n        # Step 2: Validate safety constraints\n        if not self.validate_constraints(task_plan.constraints):\n            raise ValueError("Safety constraints violated")\n\n        # Step 3: Map to ROS 2 actions\n        ros_actions = []\n        for action in task_plan.actions:\n            ros_action = self.action_mapper.map_to_ros2(action)\n            ros_actions.append(ros_action)\n\n        return ros_actions\n\n    def validate_constraints(self, constraints: List[Dict[str, Any]]) -> bool:\n        """Validate that all safety constraints are satisfied"""\n        for constraint in constraints:\n            if constraint["severity"] == "error":\n                # Implement constraint validation logic\n                if not self.check_constraint(constraint):\n                    return False\n        return True\n'})}),"\n",(0,i.jsx)(e.h3,{id:"ros-2-action-mapping",children:"ROS 2 Action Mapping"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class ROS2ActionMapper:\n    def __init__(self):\n        self.action_mapping = {\n            "navigation": self._map_navigation,\n            "manipulation": self._map_manipulation,\n            "perception": self._map_perception,\n            "communication": self._map_communication\n        }\n\n    def map_to_ros2(self, action: ActionSequence) -> Dict[str, Any]:\n        """Map action to ROS 2 service call or topic message"""\n        if action.type in self.action_mapping:\n            return self.action_mapping[action.type](action)\n        else:\n            raise ValueError(f"Unknown action type: {action.type}")\n\n    def _map_navigation(self, action: ActionSequence) -> Dict[str, Any]:\n        """Map navigation action to ROS 2 navigation2 interface"""\n        return {\n            "service": "/navigate_to_pose",\n            "request": {\n                "pose": action.parameters.get("pose", {}),\n                "behavior_tree": action.parameters.get("behavior_tree", "default")\n            }\n        }\n\n    def _map_manipulation(self, action: ActionSequence) -> Dict[str, Any]:\n        """Map manipulation action to ROS 2 manipulation interface"""\n        return {\n            "service": "/manipulation/plan",\n            "request": {\n                "target_pose": action.parameters.get("target_pose", {}),\n                "object_id": action.parameters.get("object_id", "")\n            }\n        }\n\n    def _map_perception(self, action: ActionSequence) -> Dict[str, Any]:\n        """Map perception action to ROS 2 perception interface"""\n        return {\n            "service": "/perception/detect_objects",\n            "request": {\n                "camera_name": action.parameters.get("camera", "head_camera"),\n                "object_types": action.parameters.get("object_types", [])\n            }\n        }\n\n    def _map_communication(self, action: ActionSequence) -> Dict[str, Any]:\n        """Map communication action to ROS 2 communication interface"""\n        return {\n            "topic": "/tts/text",\n            "message": {\n                "text": action.parameters.get("text", ""),\n                "voice": action.parameters.get("voice", "default")\n            }\n        }\n'})}),"\n",(0,i.jsx)(e.h2,{id:"safety-and-constraint-aware-planning",children:"Safety and Constraint-Aware Planning"}),"\n",(0,i.jsx)(e.p,{children:"Safety is paramount in humanoid robotics, especially when autonomous systems interpret natural language commands. Constraint-aware planning ensures that all planned actions comply with safety requirements."}),"\n",(0,i.jsx)(e.h3,{id:"safety-validation-framework",children:"Safety Validation Framework"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class SafetyConstraintValidator:\n    def __init__(self):\n        self.constraint_rules = {\n            "navigation": self._validate_navigation_safety,\n            "manipulation": self._validate_manipulation_safety,\n            "environmental": self._validate_environmental_safety\n        }\n\n    def validate_action(self, action: ActionSequence, current_state: Dict[str, Any]) -> bool:\n        """Validate an action against safety constraints"""\n        if action.type in self.constraint_rules:\n            return self.constraint_rules[action.type](action, current_state)\n        return True  # If no specific rule, assume safe\n\n    def _validate_navigation_safety(self, action: ActionSequence, state: Dict[str, Any]) -> bool:\n        """Validate navigation safety constraints"""\n        target_pose = action.parameters.get("pose", {})\n\n        # Check if target is in safe zone\n        if not self._is_safe_navigation_target(target_pose, state):\n            return False\n\n        # Check path for obstacles\n        if not self._is_path_clear(target_pose, state):\n            return False\n\n        return True\n\n    def _validate_manipulation_safety(self, action: ActionSequence, state: Dict[str, Any]) -> bool:\n        """Validate manipulation safety constraints"""\n        target_pose = action.parameters.get("target_pose", {})\n\n        # Check if target is within safe reach\n        if not self._is_safe_manipulation_target(target_pose, state):\n            return False\n\n        # Check if object is safe to manipulate\n        object_id = action.parameters.get("object_id", "")\n        if not self._is_safe_to_manipulate(object_id, state):\n            return False\n\n        return True\n\n    def _validate_environmental_safety(self, action: ActionSequence, state: Dict[str, Any]) -> bool:\n        """Validate environmental safety constraints"""\n        # Check environmental conditions\n        environment = state.get("environment", {})\n\n        # Check for hazardous conditions\n        if environment.get("hazard_detected", False):\n            return False\n\n        # Check for safety-critical situations\n        if environment.get("emergency", False):\n            return False\n\n        return True\n\n    def _is_safe_navigation_target(self, target_pose: Dict, state: Dict) -> bool:\n        """Check if navigation target is safe"""\n        # Implementation of safety checks\n        return True\n\n    def _is_path_clear(self, target_pose: Dict, state: Dict) -> bool:\n        """Check if path to target is clear of obstacles"""\n        # Implementation of path validation\n        return True\n\n    def _is_safe_manipulation_target(self, target_pose: Dict, state: Dict) -> bool:\n        """Check if manipulation target is within safe reach"""\n        # Implementation of reachability and safety checks\n        return True\n\n    def _is_safe_to_manipulate(self, object_id: str, state: Dict) -> bool:\n        """Check if object is safe to manipulate"""\n        # Implementation of object safety checks\n        return True\n'})}),"\n",(0,i.jsx)(e.h3,{id:"integration-with-ros-2-safety-systems",children:"Integration with ROS 2 Safety Systems"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom action_msgs.msg import GoalStatus\n\nclass SafeCognitivePlanner(Node):\n    def __init__(self):\n        super().__init__(\'safe_cognitive_planner\')\n\n        # Publishers and subscribers\n        self.plan_publisher = self.create_publisher(\n            String,\n            \'cognitive_plans\',\n            10\n        )\n\n        self.safety_status_sub = self.create_subscription(\n            String,\n            \'safety_status\',\n            self.safety_callback,\n            10\n        )\n\n        # Initialize components\n        self.planning_system = CognitivePlanningSystem()\n        self.safety_validator = SafetyConstraintValidator()\n        self.current_state = {}\n\n    def safety_callback(self, msg):\n        """Update current state based on safety system"""\n        try:\n            self.current_state = json.loads(msg.data)\n        except json.JSONDecodeError:\n            self.get_logger().warn("Invalid safety status message")\n\n    def plan_and_execute(self, command: str):\n        """Plan and execute command with safety validation"""\n        # Decompose task\n        task_plan = self.planning_system.decompose_task(command)\n\n        if task_plan.status == "failed":\n            self.get_logger().error("Task decomposition failed")\n            return\n\n        # Validate safety for each action\n        for action in task_plan.actions:\n            if not self.safety_validator.validate_action(action, self.current_state):\n                self.get_logger().error(f"Action {action.id} failed safety validation")\n                # Publish safety violation\n                safety_msg = String()\n                safety_msg.data = f"safety_violation:{action.id}"\n                self.plan_publisher.publish(safety_msg)\n                return\n\n        # If all actions pass safety validation, publish the plan\n        plan_msg = String()\n        plan_msg.data = json.dumps({\n            "plan_id": task_plan.id,\n            "actions": [action.__dict__ for action in task_plan.actions],\n            "original_command": task_plan.original_command\n        })\n\n        self.plan_publisher.publish(plan_msg)\n        self.get_logger().info(f"Published safe task plan: {task_plan.id}")\n'})}),"\n",(0,i.jsx)(e.h2,{id:"implementation-best-practices",children:"Implementation Best Practices"}),"\n",(0,i.jsx)(e.h3,{id:"error-handling-and-fallbacks",children:"Error Handling and Fallbacks"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class RobustCognitivePlanner:\n    def __init__(self):\n        self.fallback_strategies = [\n            self._use_rule_based_planning,\n            self._request_user_clarification,\n            self._execute_safe_default\n        ]\n\n    def plan_with_fallback(self, command: str):\n        """Plan with multiple fallback strategies"""\n        try:\n            # Try LLM-based planning first\n            return self.planning_system.decompose_task(command)\n        except Exception as e:\n            self.get_logger().warn(f"LLM planning failed: {e}")\n\n            # Try fallback strategies\n            for fallback in self.fallback_strategies:\n                try:\n                    result = fallback(command)\n                    if result:\n                        return result\n                except Exception as fe:\n                    self.get_logger().warn(f"Fallback failed: {fe}")\n\n        return self._safe_failure_response(command)\n'})}),"\n",(0,i.jsx)(e.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Caching"}),": Cache common command patterns and their decompositions"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Parallel Processing"}),": Process multiple commands in parallel when safe"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Model Optimization"}),": Use appropriate model sizes for real-time requirements"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Response Validation"}),": Validate LLM responses before execution"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(e.p,{children:"In this chapter, we've explored how to use LLMs for cognitive planning in humanoid robotics:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Task decomposition using LLMs to convert natural language to action sequences"}),"\n",(0,i.jsx)(e.li,{children:"ROS 2 integration patterns for translating commands to executable actions"}),"\n",(0,i.jsx)(e.li,{children:"Safety and constraint-aware planning to ensure safe robot operation"}),"\n",(0,i.jsx)(e.li,{children:"Best practices for robust and reliable cognitive planning"}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"In the next chapter, we'll combine these components into a complete end-to-end VLA architecture that orchestrates speech, planning, and execution in simulation environments."})]})}function d(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(p,{...n})}):p(n)}}}]);