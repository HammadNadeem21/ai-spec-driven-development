"use strict";(globalThis.webpackChunkfrontend_book=globalThis.webpackChunkfrontend_book||[]).push([[223],{2562(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>p,frontMatter:()=>a,metadata:()=>t,toc:()=>r});const t=JSON.parse('{"id":"modules/vla/practical-exercises","title":"Practical Exercises: VLA System","description":"This section provides hands-on exercises to reinforce the concepts learned in the Vision-Language-Action (VLA) module. These exercises will help you implement and test the complete VLA pipeline.","source":"@site/docs/modules/vla/practical-exercises.md","sourceDirName":"modules/vla","slug":"/modules/vla/practical-exercises","permalink":"/docs/modules/vla/practical-exercises","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/vla/practical-exercises.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Practical Exercises: VLA System"}}');var s=i(4848),o=i(8453);const a={sidebar_position:4,title:"Practical Exercises: VLA System"},c="VLA Practical Exercises",l={},r=[{value:"Exercise 1: Basic Voice Command Processing",id:"exercise-1-basic-voice-command-processing",level:2},{value:"Objective",id:"objective",level:3},{value:"Steps",id:"steps",level:3},{value:"Code Template",id:"code-template",level:3},{value:"Exercise 2: Cognitive Planning with LLMs",id:"exercise-2-cognitive-planning-with-llms",level:2},{value:"Objective",id:"objective-1",level:3},{value:"Steps",id:"steps-1",level:3},{value:"Code Template",id:"code-template-1",level:3},{value:"Exercise 3: Complete VLA Pipeline Integration",id:"exercise-3-complete-vla-pipeline-integration",level:2},{value:"Objective",id:"objective-2",level:3},{value:"Steps",id:"steps-2",level:3},{value:"Code Template",id:"code-template-2",level:3},{value:"Exercise 4: Simulation Integration",id:"exercise-4-simulation-integration",level:2},{value:"Objective",id:"objective-3",level:3},{value:"Steps",id:"steps-3",level:3},{value:"Testing Scenarios",id:"testing-scenarios",level:3},{value:"Exercise 5: Performance Optimization",id:"exercise-5-performance-optimization",level:2},{value:"Objective",id:"objective-4",level:3},{value:"Steps",id:"steps-4",level:3},{value:"Key Metrics to Track",id:"key-metrics-to-track",level:3},{value:"Exercise 6: Advanced Features",id:"exercise-6-advanced-features",level:2},{value:"Objective",id:"objective-5",level:3},{value:"Extensions to Implement",id:"extensions-to-implement",level:3},{value:"Assessment Criteria",id:"assessment-criteria",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"vla-practical-exercises",children:"VLA Practical Exercises"})}),"\n",(0,s.jsx)(n.p,{children:"This section provides hands-on exercises to reinforce the concepts learned in the Vision-Language-Action (VLA) module. These exercises will help you implement and test the complete VLA pipeline."}),"\n",(0,s.jsx)(n.h2,{id:"exercise-1-basic-voice-command-processing",children:"Exercise 1: Basic Voice Command Processing"}),"\n",(0,s.jsx)(n.h3,{id:"objective",children:"Objective"}),"\n",(0,s.jsx)(n.p,{children:"Implement a basic voice command processing pipeline that captures audio, converts it to text, and publishes it to a ROS 2 topic."}),"\n",(0,s.jsx)(n.h3,{id:"steps",children:"Steps"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Set up the OpenAI Whisper API configuration"}),"\n",(0,s.jsx)(n.li,{children:"Create an audio capture node that listens for voice commands"}),"\n",(0,s.jsx)(n.li,{children:"Process the audio through Whisper to convert to text"}),"\n",(0,s.jsx)(n.li,{children:"Publish the text command to a ROS 2 topic"}),"\n",(0,s.jsx)(n.li,{children:'Test with simple commands like "move forward" or "stop"'}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"code-template",children:"Code Template"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nimport speech_recognition as sr\nimport openai\nfrom std_msgs.msg import String\n\nclass VoiceCommandNode(Node):\n    def __init__(self):\n        super().__init__('voice_command_node')\n        self.publisher = self.create_publisher(String, 'voice_commands', 10)\n\n        # Initialize speech recognizer\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n\n        # Start listening loop\n        self.timer = self.create_timer(1.0, self.listen_for_command)\n\n    def listen_for_command(self):\n        \"\"\"Listen for and process voice commands\"\"\"\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n            try:\n                audio = self.recognizer.listen(source, timeout=5.0)\n                # Process with Whisper\n                result = openai.Audio.transcribe(\"whisper-1\", audio)\n\n                # Publish the command\n                msg = String()\n                msg.data = result.text\n                self.publisher.publish(msg)\n                self.get_logger().info(f'Published command: {result.text}')\n\n            except sr.WaitTimeoutError:\n                self.get_logger().info('No audio detected')\n            except Exception as e:\n                self.get_logger().error(f'Error processing audio: {e}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceCommandNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"exercise-2-cognitive-planning-with-llms",children:"Exercise 2: Cognitive Planning with LLMs"}),"\n",(0,s.jsx)(n.h3,{id:"objective-1",children:"Objective"}),"\n",(0,s.jsx)(n.p,{children:"Create a cognitive planning system that takes natural language commands and decomposes them into action sequences."}),"\n",(0,s.jsx)(n.h3,{id:"steps-1",children:"Steps"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Set up OpenAI API for LLM integration"}),"\n",(0,s.jsx)(n.li,{children:"Create a prompt template for task decomposition"}),"\n",(0,s.jsx)(n.li,{children:"Implement the task decomposition logic"}),"\n",(0,s.jsx)(n.li,{children:"Validate actions against safety constraints"}),"\n",(0,s.jsx)(n.li,{children:'Test with complex commands like "Go to the kitchen and bring me a cup"'}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"code-template-1",children:"Code Template"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import openai\nimport json\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any\n\n@dataclass\nclass Action:\n    type: str\n    parameters: Dict[str, Any]\n    dependencies: List[str]\n\nclass CognitivePlanner:\n    def __init__(self):\n        self.client = openai.OpenAI()\n\n    def decompose_task(self, command: str) -> List[Action]:\n        """Decompose a natural language command into actions"""\n        prompt = f"""\n        Convert the following command into a sequence of robot actions:\n        Command: "{command}"\n\n        Return a JSON array of actions with type and parameters.\n        Valid action types: navigation, manipulation, perception, communication\n\n        Example format:\n        [\n            {{\n                "type": "navigation",\n                "parameters": {{"target": "kitchen"}},\n                "dependencies": []\n            }},\n            {{\n                "type": "perception",\n                "parameters": {{"object": "cup"}},\n                "dependencies": ["navigation"]\n            }}\n        ]\n        """\n\n        response = self.client.chat.completions.create(\n            model="gpt-3.5-turbo",\n            messages=[{"role": "user", "content": prompt}],\n            response_format={"type": "json_object"}\n        )\n\n        result = json.loads(response.choices[0].message.content)\n        actions = []\n\n        for action_data in result:\n            action = Action(\n                type=action_data["type"],\n                parameters=action_data["parameters"],\n                dependencies=action_data.get("dependencies", [])\n            )\n            actions.append(action)\n\n        return actions\n\n# Test the planner\nplanner = CognitivePlanner()\nactions = planner.decompose_task("Go to the kitchen and bring me a red cup")\nprint(f"Decomposed {len(actions)} actions")\nfor i, action in enumerate(actions):\n    print(f"{i+1}. {action.type}: {action.parameters}")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"exercise-3-complete-vla-pipeline-integration",children:"Exercise 3: Complete VLA Pipeline Integration"}),"\n",(0,s.jsx)(n.h3,{id:"objective-2",children:"Objective"}),"\n",(0,s.jsx)(n.p,{children:"Integrate voice processing, cognitive planning, and action execution into a complete system."}),"\n",(0,s.jsx)(n.h3,{id:"steps-2",children:"Steps"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Create a VLA orchestrator node"}),"\n",(0,s.jsx)(n.li,{children:"Connect voice input to cognitive planning"}),"\n",(0,s.jsx)(n.li,{children:"Connect planning output to action execution"}),"\n",(0,s.jsx)(n.li,{children:"Implement safety validation between components"}),"\n",(0,s.jsx)(n.li,{children:"Test the complete pipeline with end-to-end scenarios"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"code-template-2",children:"Code Template"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport json\n\nclass VLAOrchestrator(Node):\n    def __init__(self):\n        super().__init__(\'vla_orchestrator\')\n\n        # Subscribers\n        self.voice_sub = self.create_subscription(\n            String, \'voice_commands\', self.voice_callback, 10\n        )\n\n        self.action_status_sub = self.create_subscription(\n            String, \'action_status\', self.action_status_callback, 10\n        )\n\n        # Publishers\n        self.plan_pub = self.create_publisher(String, \'action_plans\', 10)\n        self.status_pub = self.create_publisher(String, \'vla_status\', 10)\n\n        # Initialize components\n        self.planner = CognitivePlanner()\n\n        # Active plan tracking\n        self.active_plan = None\n        self.plan_status = {}\n\n    def voice_callback(self, msg: String):\n        """Process incoming voice commands"""\n        command = msg.data\n        self.get_logger().info(f\'Received command: {command}\')\n\n        # Decompose task\n        try:\n            actions = self.planner.decompose_task(command)\n\n            # Create plan\n            plan = {\n                "command": command,\n                "actions": [\n                    {\n                        "id": f"action_{i}",\n                        "type": action.type,\n                        "parameters": action.parameters,\n                        "dependencies": action.dependencies\n                    }\n                    for i, action in enumerate(actions)\n                ]\n            }\n\n            # Publish plan\n            plan_msg = String()\n            plan_msg.data = json.dumps(plan)\n            self.plan_pub.publish(plan_msg)\n\n            # Track plan\n            self.active_plan = plan\n            self.plan_status = {action["id"]: "pending" for action in plan["actions"]}\n\n            self.get_logger().info(f\'Published plan with {len(actions)} actions\')\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing command: {e}\')\n\n    def action_status_callback(self, msg: String):\n        """Update plan status based on action execution"""\n        try:\n            status = json.loads(msg.data)\n            action_id = status.get("action_id")\n            status_value = status.get("status")\n\n            if action_id in self.plan_status:\n                self.plan_status[action_id] = status_value\n\n                # Check if plan is complete\n                if all(s == "completed" for s in self.plan_status.values()):\n                    self.get_logger().info(\'Plan completed successfully!\')\n                    self.active_plan = None\n                    self.plan_status = {}\n\n        except Exception as e:\n            self.get_logger().error(f\'Error updating status: {e}\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    orchestrator = VLAOrchestrator()\n    rclpy.spin(orchestrator)\n    orchestrator.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"exercise-4-simulation-integration",children:"Exercise 4: Simulation Integration"}),"\n",(0,s.jsx)(n.h3,{id:"objective-3",children:"Objective"}),"\n",(0,s.jsx)(n.p,{children:"Integrate the VLA system with a simulation environment to test in a safe, controlled setting."}),"\n",(0,s.jsx)(n.h3,{id:"steps-3",children:"Steps"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Set up a basic simulation environment (Gazebo or Isaac Sim)"}),"\n",(0,s.jsx)(n.li,{children:"Connect VLA system to simulation state publisher"}),"\n",(0,s.jsx)(n.li,{children:"Implement simulation-specific action execution"}),"\n",(0,s.jsx)(n.li,{children:"Test complete scenarios in simulation"}),"\n",(0,s.jsx)(n.li,{children:"Validate safety constraints in simulated environment"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"testing-scenarios",children:"Testing Scenarios"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple Navigation"}),': "Go to the table" - Test basic navigation planning']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Object Interaction"}),': "Pick up the red block" - Test perception and manipulation']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Complex Task"}),': "Go to kitchen, find a cup, and bring it to me" - Test full pipeline']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety Validation"}),': "Go near the stairs" - Test constraint validation']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Error Recovery"}),': "Go to unknown location" - Test fallback mechanisms']}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"exercise-5-performance-optimization",children:"Exercise 5: Performance Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"objective-4",children:"Objective"}),"\n",(0,s.jsx)(n.p,{children:"Optimize the VLA system for real-time performance and reliability."}),"\n",(0,s.jsx)(n.h3,{id:"steps-4",children:"Steps"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Implement caching for common command patterns"}),"\n",(0,s.jsx)(n.li,{children:"Add timeout handling for LLM calls"}),"\n",(0,s.jsx)(n.li,{children:"Optimize audio processing for real-time performance"}),"\n",(0,s.jsx)(n.li,{children:"Implement graceful degradation when components fail"}),"\n",(0,s.jsx)(n.li,{children:"Add comprehensive logging and monitoring"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"key-metrics-to-track",children:"Key Metrics to Track"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Voice processing latency (< 2 seconds)"}),"\n",(0,s.jsx)(n.li,{children:"Planning response time (< 5 seconds)"}),"\n",(0,s.jsx)(n.li,{children:"Action execution success rate (> 80%)"}),"\n",(0,s.jsx)(n.li,{children:"System uptime (> 95% during testing)"}),"\n",(0,s.jsx)(n.li,{children:"Safety constraint violation rate (< 1%)"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"exercise-6-advanced-features",children:"Exercise 6: Advanced Features"}),"\n",(0,s.jsx)(n.h3,{id:"objective-5",children:"Objective"}),"\n",(0,s.jsx)(n.p,{children:"Extend the VLA system with advanced capabilities."}),"\n",(0,s.jsx)(n.h3,{id:"extensions-to-implement",children:"Extensions to Implement"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-turn Conversations"}),": Handle follow-up commands and context"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Learning from Corrections"}),": Adapt behavior based on user feedback"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-modal Input"}),": Combine voice with visual or gesture input"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Distributed Processing"}),": Handle processing across multiple nodes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Adaptive Planning"}),": Modify plans based on real-time feedback"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"assessment-criteria",children:"Assessment Criteria"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Successful implementation of basic voice processing pipeline"}),"\n",(0,s.jsx)(n.li,{children:"Correct decomposition of natural language into action sequences"}),"\n",(0,s.jsx)(n.li,{children:"Proper safety validation and constraint checking"}),"\n",(0,s.jsx)(n.li,{children:"End-to-end functionality demonstration"}),"\n",(0,s.jsx)(n.li,{children:"Performance optimization and error handling"}),"\n",(0,s.jsx)(n.li,{children:"Clean, well-documented code following ROS 2 conventions"}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>a,x:()=>c});var t=i(6540);const s={},o=t.createContext(s);function a(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);