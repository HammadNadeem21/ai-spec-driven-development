"use strict";(globalThis.webpackChunkfrontend_book=globalThis.webpackChunkfrontend_book||[]).push([[535],{5684(e,n,i){i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>c,default:()=>p,frontMatter:()=>t,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"modules/vla/chapter-1-voice-to-action","title":"Chapter 1: Voice-to-Action with Speech Models","description":"Overview","source":"@site/docs/modules/vla/chapter-1-voice-to-action.md","sourceDirName":"modules/vla","slug":"/modules/vla/chapter-1-voice-to-action","permalink":"/docs/modules/vla/chapter-1-voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/vla/chapter-1-voice-to-action.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Chapter 1: Voice-to-Action with Speech Models"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3 - Nav2 for Humanoids","permalink":"/docs/modules/isaac-ai-brain/chapter-3-nav2-humanoids"},"next":{"title":"Chapter 2: Language-Driven Cognitive Planning","permalink":"/docs/modules/vla/chapter-2-cognitive-planning"}}');var r=i(4848),s=i(8453);const t={sidebar_position:1,title:"Chapter 1: Voice-to-Action with Speech Models"},c="Chapter 1: Voice-to-Action with Speech Models",a={},l=[{value:"Overview",id:"overview",level:2},{value:"Voice Command Pipelines for Robots",id:"voice-command-pipelines-for-robots",level:2},{value:"Key Considerations",id:"key-considerations",level:3},{value:"Using OpenAI Whisper for Speech-to-Text",id:"using-openai-whisper-for-speech-to-text",level:2},{value:"Whisper Integration Architecture",id:"whisper-integration-architecture",level:3},{value:"Configuration for Robot Environments",id:"configuration-for-robot-environments",level:3},{value:"Feeding Voice Commands into ROS 2 Systems",id:"feeding-voice-commands-into-ros-2-systems",level:2},{value:"ROS 2 Integration Pattern",id:"ros-2-integration-pattern",level:3},{value:"Topic Architecture Considerations",id:"topic-architecture-considerations",level:3},{value:"Implementation Best Practices",id:"implementation-best-practices",level:2},{value:"Error Handling and Fallbacks",id:"error-handling-and-fallbacks",level:3},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-1-voice-to-action-with-speech-models",children:"Chapter 1: Voice-to-Action with Speech Models"})}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"This chapter introduces the fundamental concepts of voice command processing for humanoid robots, focusing on converting spoken commands into actionable instructions. We'll explore how to create robust voice interfaces that can operate in real-world environments and integrate seamlessly with robot control systems."}),"\n",(0,r.jsx)(n.h2,{id:"voice-command-pipelines-for-robots",children:"Voice Command Pipelines for Robots"}),"\n",(0,r.jsx)(n.p,{children:"Voice command processing in robotics involves several key components that work together to transform human speech into robot actions:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Audio Capture"}),": Capturing clear audio input from the environment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Preprocessing"}),": Filtering noise and enhancing signal quality"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Speech-to-Text"}),": Converting audio to text using advanced models"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Command Parsing"}),": Interpreting the text to extract actionable commands"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS 2 Integration"}),": Publishing commands to appropriate topics for robot execution"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"key-considerations",children:"Key Considerations"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Environmental Noise"}),": Robots operate in various acoustic environments requiring adaptive filtering"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-time Processing"}),": Commands must be processed with minimal latency for responsive interaction"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Accuracy"}),": High confidence in speech recognition is crucial for robot safety"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-language Support"}),": Consideration for diverse user language requirements"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"using-openai-whisper-for-speech-to-text",children:"Using OpenAI Whisper for Speech-to-Text"}),"\n",(0,r.jsx)(n.p,{children:"OpenAI Whisper is a state-of-the-art speech recognition model that provides exceptional accuracy across multiple languages and accents. In this section, we'll implement Whisper integration for robot voice command processing."}),"\n",(0,r.jsx)(n.h3,{id:"whisper-integration-architecture",children:"Whisper Integration Architecture"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import openai\nimport speech_recognition as sr\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass VoiceCommand:\n    """Represents a voice command captured from the user"""\n    id: str\n    transcript: str\n    confidence: float\n    timestamp: str\n    language: str\n\nclass VoiceCommandPipeline:\n    def __init__(self):\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n        # Configure for robot environment\n        self.recognizer.energy_threshold = 4000  # Adjust for ambient noise\n        self.recognizer.dynamic_energy_threshold = True\n\n    def capture_and_transcribe(self) -> Optional[VoiceCommand]:\n        """Capture audio from microphone and transcribe using Whisper"""\n        try:\n            with self.microphone as source:\n                # Listen for audio with timeout\n                audio = self.recognizer.listen(source, timeout=5.0)\n\n            # Use Whisper for transcription (requires OpenAI API key)\n            transcript = openai.Audio.transcribe(\n                "whisper-1",\n                audio,\n                response_format="verbose_json"\n            )\n\n            # Create voice command object\n            command = VoiceCommand(\n                id=f"cmd_{int(time.time())}",\n                transcript=transcript.text,\n                confidence=transcript.confidence,\n                timestamp=transcript.duration,\n                language=transcript.language\n            )\n\n            return command\n        except sr.WaitTimeoutError:\n            print("No audio detected within timeout period")\n            return None\n        except Exception as e:\n            print(f"Error in voice processing: {e}")\n            return None\n'})}),"\n",(0,r.jsx)(n.h3,{id:"configuration-for-robot-environments",children:"Configuration for Robot Environments"}),"\n",(0,r.jsx)(n.p,{children:"Robots operate in diverse acoustic environments, so Whisper configuration must account for:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Microphone Quality"}),": Adjust sensitivity based on hardware capabilities"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Background Noise"}),": Implement adaptive noise cancellation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Distance to Speaker"}),": Account for varying audio quality based on proximity"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Acoustic Environment"}),": Different settings (indoor/outdoor, quiet/noisy) require different processing"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"feeding-voice-commands-into-ros-2-systems",children:"Feeding Voice Commands into ROS 2 Systems"}),"\n",(0,r.jsx)(n.p,{children:"Once voice commands are processed, they need to be integrated into the ROS 2 ecosystem for robot execution. This involves publishing to appropriate topics that downstream systems can consume."}),"\n",(0,r.jsx)(n.h3,{id:"ros-2-integration-pattern",children:"ROS 2 Integration Pattern"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import AudioData\n\nclass VoiceCommandPublisher(Node):\n    def __init__(self):\n        super().__init__('voice_command_publisher')\n\n        # Publisher for processed text commands\n        self.command_publisher = self.create_publisher(\n            String,\n            'voice_commands',\n            10\n        )\n\n        # Publisher for raw audio data (if needed for local processing)\n        self.audio_publisher = self.create_publisher(\n            AudioData,\n            'raw_audio',\n            10\n        )\n\n        # Timer for periodic voice processing\n        self.timer = self.create_timer(1.0, self.process_voice_command)\n\n    def publish_command(self, command: VoiceCommand):\n        \"\"\"Publish processed voice command to ROS 2 topic\"\"\"\n        msg = String()\n        msg.data = command.transcript\n        self.command_publisher.publish(msg)\n\n        self.get_logger().info(f'Published command: {command.transcript}')\n\n# Integration with the voice pipeline\ndef main(args=None):\n    rclpy.init(args=args)\n\n    # Initialize voice command pipeline\n    voice_pipeline = VoiceCommandPipeline()\n\n    # Initialize ROS 2 publisher\n    voice_publisher = VoiceCommandPublisher()\n\n    # Process and publish commands in a loop\n    while rclpy.ok():\n        command = voice_pipeline.capture_and_transcribe()\n        if command and command.confidence > 0.7:  # Confidence threshold\n            voice_publisher.publish_command(command)\n\n        rclpy.spin_once(voice_publisher, timeout_sec=0.1)\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"topic-architecture-considerations",children:"Topic Architecture Considerations"}),"\n",(0,r.jsx)(n.p,{children:"When designing the ROS 2 topic architecture for voice commands:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Command Topic"}),": ",(0,r.jsx)(n.code,{children:"/voice_commands"})," - for processed text commands"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Audio Topic"}),": ",(0,r.jsx)(n.code,{children:"/raw_audio"})," - for raw audio data if local processing is needed"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Status Topic"}),": ",(0,r.jsx)(n.code,{children:"/voice_status"})," - for feedback on voice processing status"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Configuration Service"}),": ",(0,r.jsx)(n.code,{children:"/voice_config"})," - for adjusting voice processing parameters"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"implementation-best-practices",children:"Implementation Best Practices"}),"\n",(0,r.jsx)(n.h3,{id:"error-handling-and-fallbacks",children:"Error Handling and Fallbacks"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class RobustVoiceProcessor:\n    def __init__(self):\n        self.fallback_enabled = True\n        self.max_retries = 3\n\n    def process_with_fallback(self, audio_data):\n        """Process audio with multiple fallback strategies"""\n        # Try Whisper first\n        result = self.try_whisper(audio_data)\n        if result and result.confidence > 0.8:\n            return result\n\n        # Fallback to local speech recognition\n        if self.fallback_enabled:\n            result = self.try_local_recognition(audio_data)\n            return result\n\n        return None\n'})}),"\n",(0,r.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Batch Processing"}),": Process multiple audio segments efficiently"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Caching"}),": Cache common command patterns for faster recognition"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Edge Computing"}),": Consider on-device processing for latency-sensitive applications"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Resource Management"}),": Monitor CPU and memory usage during processing"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"In this chapter, we've explored the fundamentals of voice command processing for humanoid robots. We've covered:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"The complete voice command pipeline from capture to execution"}),"\n",(0,r.jsx)(n.li,{children:"OpenAI Whisper integration for high-accuracy speech-to-text"}),"\n",(0,r.jsx)(n.li,{children:"ROS 2 integration patterns for command publishing"}),"\n",(0,r.jsx)(n.li,{children:"Best practices for robust voice processing in robot environments"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"In the next chapter, we'll explore how to use LLMs for cognitive planning and task decomposition, transforming natural language commands into executable robot actions."})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>t,x:()=>c});var o=i(6540);const r={},s=o.createContext(r);function t(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);